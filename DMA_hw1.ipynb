{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FUNCTION DEFINITIONS ==========\n",
    "def calc_proportion(array_TF):\n",
    "    return sum(array_TF)/len(array_TF)\n",
    "\n",
    "def calc_zscore(phat, p, n_f):\n",
    "    z_score_f = (phat-p)/(p*(1-p)/n_f)**0.5   ##### Replace None with formula\n",
    "    return z_score_f\n",
    "\n",
    "def get_z_crit_value(alpha_f, num_sides_f):\n",
    "    z_crit_value_f = norm.ppf((1-alpha_f/num_sides_f,0,1))[0]  ##### Replace None with formula; hint: use norm.ppf package\n",
    "    return z_crit_value_f\n",
    "\n",
    "def get_p_value(zscore_f, num_sides_f):\n",
    "    p_value_f = (1-norm.cdf(zscore_f,0,1)) * num_sides_f ##### Replace None with formula; hint: use norm.cdf package\n",
    "    return p_value_f\n",
    "\n",
    "def reject_null(variantA_outcomes_f, variantB_outcomes_f, alpha_f, num_sides_f):\n",
    "    p_hat_f = calc_proportion(variantB_outcomes_f)\n",
    "    p_f = calc_proportion(variantA_outcomes_f)\n",
    "    n_f = len(variantB_outcomes_f)\n",
    "    z_score = calc_zscore(p_hat_f, p_f, n_f)\n",
    "    p_value = get_p_value(z_score, num_sides_f)\n",
    "    z_crit = get_z_crit_value(alpha_f, num_sides_f)\n",
    "    reject_null_TF_f = True if (abs(z_score) >= abs(z_crit)) else False  ##### Replace None with formula. This should result in a boolean variable (True or False). You can check the variable type in the console with the command: \"type(reject_null_TF_f)\"\n",
    "    return reject_null_TF_f, z_score, p_value\n",
    "\n",
    "def calc_optimal_sample_size(p0_f, mde_f, alpha_f, power_f):\n",
    "    t_alpha2 = abs(norm.ppf(alpha_f/2))\n",
    "    t_beta = abs(norm.ppf(1-power_f))\n",
    "    p1_f = p0_f + mde_f\n",
    "    p_avg = (p0_f + (p0_f + mde_f))/2\n",
    "    sample_size = (t_alpha2*(2*p_avg*(1-p_avg))**0.5+ (t_beta)*(p0_f*(1-p0_f)+p1_f*(1-p1_f))**0.5)**2/mde_f**2  ########## Replace None with formula\n",
    "    return sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DECLARE PARAMETERS ==========\n",
    "trial_start_date = datetime.date(year=2020, month=8, day=1)\n",
    "\n",
    "# ========== LOAD DATA ==========\n",
    "df = pd.read_csv('AB_trial_data.csv')\n",
    "df.date = pd.to_datetime(df.date, format='%Y-%m-%d')    # parse string format\n",
    "df.date = df.date.apply(lambda x: datetime.date(year=x.year, month=x.month, day=x.day)) # convert to standard (non-pandas) format for comparison against other dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For month beginning 2020-08-01, Variant A had 5000 exposures (15.1%) and Variant B had 5000 exposures (16.5%)\n"
     ]
    }
   ],
   "source": [
    "# ========== ANALYSES ==========\n",
    "# ----- Get summary stats -----\n",
    "df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "df_summary = df[['year', 'month', 'Variant', 'id', 'purchase_TF']].groupby(['year', 'month', 'Variant']).agg({'id': 'count', 'purchase_TF': 'sum'}).rename(columns={'id': 'num_exposures', 'purchase_TF': 'num_bookings'})\n",
    "df_summary['conv_rate'] = df_summary['num_bookings']/df_summary['num_exposures']\n",
    "perc_vA = df_summary.loc[(trial_start_date.year, trial_start_date.month, 'A'), 'num_bookings'] / df_summary.loc[(trial_start_date.year, trial_start_date.month, 'A'), 'num_exposures']\n",
    "perc_vB = df_summary.loc[(trial_start_date.year, trial_start_date.month, 'B'), 'num_bookings'] / df_summary.loc[(trial_start_date.year, trial_start_date.month, 'B'), 'num_exposures']\n",
    "print('For month beginning %s, Variant A had %d exposures (%3.1f%%) and Variant B had %d exposures (%3.1f%%)' % (trial_start_date, int(df_summary.loc[(trial_start_date.year, trial_start_date.month, 'A'), 'num_exposures']), perc_vA*100, int(df_summary.loc[(trial_start_date.year, trial_start_date.month, 'B'), 'num_exposures']), perc_vB*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rate for Variant A: 15.1%\n",
      "Conversion rate for Variant B: 16.5%\n",
      "Using all Variant B, reject null T/F?: True\n",
      "z-score = 2.81 and p-value = 0.5%\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "alpha = 0.05    # significance level\n",
    "num_sides = 2   # one-sided=1 or two-sided=2 test\n",
    "\n",
    "# --- choose to use all data or trial period data only by commenting and uncommenting the \"ALL DATA\" and \"TRIAL DATA ONLY\" sections\n",
    "# ALL DATA\n",
    "#variantA_outcomes = df.loc[df['Variant'] == 'A', 'purchase_TF']\n",
    "#variantB_outcomes = df.loc[df['Variant'] == 'B', 'purchase_TF']\n",
    "\n",
    "# TRIAL DATA ONLY\n",
    "variantA_outcomes = df.loc[((df['Variant'] == 'A') & (df.date >= trial_start_date)), 'purchase_TF']\n",
    "variantB_outcomes = df.loc[((df['Variant'] == 'B') & (df.date >= trial_start_date)), 'purchase_TF']\n",
    "\n",
    "# --- conduct tests\n",
    "reject_null_test, z_score, p_value = reject_null(variantA_outcomes, variantB_outcomes, alpha, num_sides)\n",
    "print('Conversion rate for Variant A: %3.1f%%' % (calc_proportion(variantA_outcomes)*100))\n",
    "print('Conversion rate for Variant B: %3.1f%%' % (calc_proportion(variantB_outcomes)*100))\n",
    "print('Using all Variant B, reject null T/F?: %s' % reject_null_test)\n",
    "print('z-score = %3.2f and p-value = %3.1f%%' % (z_score, p_value*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Question 2 -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "alpha = 0.05    # significance level\n",
    "power = 0.80    # power of test\n",
    "mde = 0.03      # desired minimum detectable effect\n",
    "num_sides = 2   # one-sided or two-sided test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure data\n",
    "variantA_outcomes_control = df.loc[(df['Variant'] == 'A') & (df.date < trial_start_date), 'purchase_TF']\n",
    "variantA_outcomes = df.loc[(df['Variant'] == 'A') & (df.date >= trial_start_date), 'purchase_TF']\n",
    "variantB_outcomes = df.loc[(df['Variant'] == 'B') & (df.date >= trial_start_date), 'purchase_TF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal sample size is 2406 \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-cbd47f01da19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mt_perc_of_trial_data_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m##### Replace None with the formula for what percent of the trial data should be used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mt_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariantB_outcomes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_perc_of_trial_data_to_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m##### No changes needed, but think about why we're using a min() function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mt_reject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_z_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_p_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreject_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariantA_outcomes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# --- Calculate and display the optimal sample size\n",
    "p0 = calc_proportion(variantA_outcomes_control)     # this would be the baseline prior to starting a test since we would only have historical information\n",
    "n_star = int(np.ceil(calc_optimal_sample_size(p0, mde, alpha, power)))    # optimal sample size; rounding up using np.ceil because sample must be a whole number that is *at least* as large as the optimal size\n",
    "num_samples = 10\n",
    "\n",
    "print('The optimal sample size is %d ' % n_star)\n",
    "if n_star > variantB_outcomes.shape[0]:\n",
    "    print('Warning: optimal sample size is larger than number of observations.')\n",
    "    \n",
    "    \n",
    "\n",
    "# --- conduct test for n samples of the optimal size\n",
    "# initialize list to store variables from each loop iteration\n",
    "variantB_outcomes_samples = list()  # this will store the data samples\n",
    "reject_null_list = list()   # this will store the results of the significance tests\n",
    "z_score_list = list()   # this still store the z-scores from each test\n",
    "p_value_list = list()   # this will store the p-values from each test\n",
    "for i in range(0, num_samples):\n",
    "    t_perc_of_trial_data_to_use = None  ##### Replace None with the formula for what percent of the trial data should be used\n",
    "    t_sample = variantB_outcomes.sample(frac=min(t_perc_of_trial_data_to_use, 1))   ##### No changes needed, but think about why we're using a min() function here\n",
    "    t_reject, t_z_score, t_p_value = reject_null(variantA_outcomes, t_sample, alpha, num_sides)\n",
    "\n",
    "    variantB_outcomes_samples.append(list(t_sample))    ##### No changes needed, but investigate what the append function is doing here\n",
    "    ##### Add lines here to update the remainder of the lists initilaized before this loop\n",
    "\n",
    "print(\"For %d samples of optimal sample size %d, %3.2f%% rejected the null\" % (num_samples, n_star, calc_proportion(reject_null_list)*100))\n",
    "\n",
    "df_sample_summary = pd.DataFrame(data={'sample number': list(range(0, num_samples)), 'z_score': z_score_list, 'p_value': p_value_list})\n",
    "print(df_sample_summary[['sample number', 'z_score', 'p_value']])\n",
    "df_sample_summary.to_csv('sample_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
